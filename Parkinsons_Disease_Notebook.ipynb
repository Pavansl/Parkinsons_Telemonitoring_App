{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6fc87e",
   "metadata": {},
   "source": [
    "# Predictive Modelling of Parkinsonâ€™s Disease Severity Using Telemonitoring Voice Data: A Machine Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39e8f2",
   "metadata": {},
   "source": [
    "### Installation Needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c2645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\ananconda_may\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\ananconda_may\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/150.0 MB 2.4 MB/s eta 0:01:03\n",
      "   ---------------------------------------- 1.6/150.0 MB 4.2 MB/s eta 0:00:36\n",
      "    --------------------------------------- 2.6/150.0 MB 4.6 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 3.9/150.0 MB 5.0 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 5.0/150.0 MB 5.2 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 6.6/150.0 MB 5.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 7.6/150.0 MB 5.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 7.9/150.0 MB 5.1 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 9.2/150.0 MB 5.0 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 10.0/150.0 MB 5.0 MB/s eta 0:00:29\n",
      "   --- ------------------------------------ 11.3/150.0 MB 5.0 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 12.3/150.0 MB 5.0 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 13.4/150.0 MB 5.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 14.4/150.0 MB 5.1 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 15.5/150.0 MB 5.0 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 16.3/150.0 MB 5.0 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 16.5/150.0 MB 4.8 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 17.0/150.0 MB 4.7 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 17.6/150.0 MB 4.5 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 17.8/150.0 MB 4.4 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 18.4/150.0 MB 4.3 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 19.1/150.0 MB 4.2 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 19.9/150.0 MB 4.2 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 21.0/150.0 MB 4.2 MB/s eta 0:00:31\n",
      "   ----- ---------------------------------- 21.8/150.0 MB 4.2 MB/s eta 0:00:31\n",
      "   ------ --------------------------------- 23.1/150.0 MB 4.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 24.1/150.0 MB 4.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 24.9/150.0 MB 4.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 25.7/150.0 MB 4.3 MB/s eta 0:00:30\n",
      "   ------ --------------------------------- 26.2/150.0 MB 4.3 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 27.0/150.0 MB 4.2 MB/s eta 0:00:30\n",
      "   ------- -------------------------------- 27.5/150.0 MB 4.2 MB/s eta 0:00:30\n",
      "   ------- -------------------------------- 28.0/150.0 MB 4.1 MB/s eta 0:00:30\n",
      "   ------- -------------------------------- 28.8/150.0 MB 4.1 MB/s eta 0:00:30\n",
      "   ------- -------------------------------- 29.9/150.0 MB 4.1 MB/s eta 0:00:29\n",
      "   -------- ------------------------------- 30.7/150.0 MB 4.1 MB/s eta 0:00:29\n",
      "   -------- ------------------------------- 31.5/150.0 MB 4.1 MB/s eta 0:00:29\n",
      "   -------- ------------------------------- 32.0/150.0 MB 4.1 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 32.5/150.0 MB 4.0 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 33.0/150.0 MB 4.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 33.8/150.0 MB 4.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 34.3/150.0 MB 4.0 MB/s eta 0:00:30\n",
      "   --------- ------------------------------ 35.1/150.0 MB 4.0 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 35.9/150.0 MB 4.0 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 37.2/150.0 MB 4.0 MB/s eta 0:00:29\n",
      "   ---------- ----------------------------- 38.3/150.0 MB 4.0 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 39.3/150.0 MB 4.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 40.9/150.0 MB 4.1 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 42.2/150.0 MB 4.2 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 43.3/150.0 MB 4.2 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 44.6/150.0 MB 4.2 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 45.9/150.0 MB 4.3 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 46.7/150.0 MB 4.2 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 48.0/150.0 MB 4.3 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 49.0/150.0 MB 4.3 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 50.1/150.0 MB 4.3 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 51.1/150.0 MB 4.3 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 52.2/150.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 53.5/150.0 MB 4.4 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 54.8/150.0 MB 4.4 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 55.8/150.0 MB 4.4 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 57.1/150.0 MB 4.4 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 58.5/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 59.5/150.0 MB 4.5 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 60.6/150.0 MB 4.5 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 61.9/150.0 MB 4.5 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 63.4/150.0 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 65.3/150.0 MB 4.6 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 66.8/150.0 MB 4.7 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 68.9/150.0 MB 4.7 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 71.3/150.0 MB 4.8 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 72.6/150.0 MB 4.9 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 74.7/150.0 MB 4.9 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 77.3/150.0 MB 5.0 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 79.4/150.0 MB 5.1 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 82.1/150.0 MB 5.2 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 5.3 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 5.3 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 83.9/150.0 MB 5.1 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 5.2 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 87.6/150.0 MB 5.2 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 89.9/150.0 MB 5.3 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 92.3/150.0 MB 5.4 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 94.4/150.0 MB 5.4 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 96.2/150.0 MB 5.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 98.6/150.0 MB 5.5 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 100.1/150.0 MB 5.5 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 102.8/150.0 MB 5.6 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 104.9/150.0 MB 5.7 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 107.5/150.0 MB 5.8 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 109.8/150.0 MB 5.8 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 111.4/150.0 MB 5.8 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 113.0/150.0 MB 5.8 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 114.0/150.0 MB 5.8 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 115.3/150.0 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 117.4/150.0 MB 5.9 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 119.0/150.0 MB 5.9 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 121.4/150.0 MB 6.0 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 123.5/150.0 MB 6.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 125.0/150.0 MB 6.0 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 127.7/150.0 MB 6.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 130.3/150.0 MB 6.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 132.9/150.0 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 135.3/150.0 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 137.4/150.0 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 139.2/150.0 MB 6.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 142.1/150.0 MB 6.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 144.2/150.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  146.3/150.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  148.6/150.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Collecting shap\n",
      "  Downloading shap-0.48.0-cp312-cp312-win_amd64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in d:\\ananconda_may\\lib\\site-packages (from shap) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\ananconda_may\\lib\\site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\ananconda_may\\lib\\site-packages (from shap) (1.5.1)\n",
      "Requirement already satisfied: pandas in d:\\ananconda_may\\lib\\site-packages (from shap) (2.2.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in d:\\ananconda_may\\lib\\site-packages (from shap) (4.66.5)\n",
      "Requirement already satisfied: packaging>20.9 in d:\\ananconda_may\\lib\\site-packages (from shap) (24.1)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numba>=0.54 in d:\\ananconda_may\\lib\\site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in d:\\ananconda_may\\lib\\site-packages (from shap) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\ananconda_may\\lib\\site-packages (from shap) (4.11.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in d:\\ananconda_may\\lib\\site-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: colorama in d:\\ananconda_may\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ananconda_may\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ananconda_may\\lib\\site-packages (from pandas->shap) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ananconda_may\\lib\\site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\ananconda_may\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\ananconda_may\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ananconda_may\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Downloading shap-0.48.0-cp312-cp312-win_amd64.whl (545 kB)\n",
      "   ---------------------------------------- 0.0/545.3 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/545.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 545.3/545.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.48.0 slicer-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install shap\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a3d45",
   "metadata": {},
   "source": [
    "### Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import ttest_ind, pearsonr\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, cross_val_score, GridSearchCV, StratifiedGroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import (r2_score, mean_squared_error, accuracy_score, precision_score,\n",
    "                             f1_score, classification_report, ConfusionMatrixDisplay, confusion_matrix)\n",
    "\n",
    "import shap\n",
    "from shap import summary_plot\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a87ee7",
   "metadata": {},
   "source": [
    "## Data Inspection and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7229f4",
   "metadata": {},
   "source": [
    "### Data Loading and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588072ff",
   "metadata": {},
   "source": [
    "This section ensures the dataset is correctly loaded and clean before moving to analysis.  \n",
    "We perform the following checks:\n",
    "\n",
    "- Confirm the file exists before loading\n",
    "- Load the dataset into memory\n",
    "- Display the first and last few rows to inspect structure\n",
    "- Show missing value counts for each column\n",
    "- Review summary statistics using 'describe()'\n",
    "- Identify and display any duplicate rows\n",
    "- Validate that:\n",
    "  - 'age' values are between 30 and 100\n",
    "  - 'sex' values are either 0 or 1\n",
    "- Check for negative values in numeric columns\n",
    "\n",
    "Negative values are flagged because features like 'Jitter', 'Shimmer', and clinical scores should not be negative. These values may result from sensor noise, data entry issues, or corruption.  \n",
    "If the number of negative rows is very small (less than 1% of total), we remove them to improve data integrity without affecting dataset balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate_data(file_path):\n",
    "    # 1. Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} not found.\")\n",
    "\n",
    "    # 2. Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"=== File loaded successfully ===\")\n",
    "    \n",
    "    # 3. Display head\n",
    "    print(\"\\n=== First 5 rows ===\")\n",
    "    display(df.head())  \n",
    "\n",
    "    # 4. Check for missing values\n",
    "    print(\"\\n=== Missing values ===\")\n",
    "    display(df.isnull().sum())\n",
    "\n",
    "    # 5. Descriptive statistics\n",
    "    print(\"\\n=== Descriptive statistics ===\")\n",
    "    display(df.describe())\n",
    "\n",
    "    # 6. Check for duplicate rows\n",
    "    print(\"\\n=== Checking for duplicate rows ===\")\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    print(f\"Total duplicate rows: {len(duplicate_rows)}\")\n",
    "    if not duplicate_rows.empty:\n",
    "        display(duplicate_rows.head())\n",
    "\n",
    "    # 8. Value range checks\n",
    "    assert df['age'].between(30, 100).all(), \"Unexpected values in 'age' column\"\n",
    "    assert df['sex'].isin([0, 1]).all(), \"Invalid values in 'sex' column\"\n",
    "    \n",
    "    # 7. Checking for multiple signal readings from one telemonitoring session\n",
    "    print(\"\\n=== Multiple measures check ===\\n\")\n",
    "    dup_check = df.groupby(['subject#', 'test_time']).size()\n",
    "    true_duplicates = dup_check[dup_check > 1]\n",
    "    print(f\"Multiple measures at the same time: {len(true_duplicates)}\")\n",
    "\n",
    "    # 8. Check and remove negative values (if negligible)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    negative_mask = (df[numeric_cols] < 0)\n",
    "    negative_counts = negative_mask.sum()\n",
    "    total_negatives = negative_mask.values.sum()\n",
    "\n",
    "    print(\"\\n=== Negative value counts ===\")\n",
    "    display(negative_counts[negative_counts > 0])\n",
    "\n",
    "    if total_negatives / df.shape[0] < 0.01:\n",
    "        df = df[~negative_mask.any(axis=1)]\n",
    "        print(f\"\\nDropped {total_negatives} rows with negative values (negligible share).\")\n",
    "\n",
    "    print(\"\\n=== Validation complete ===\\n\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and validate data ===\n",
    "DATA_FILE = \"parkinsons_updrs.data\"  # Change to your actual filename\n",
    "df = load_and_validate_data(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd8881",
   "metadata": {},
   "source": [
    "### Group-Aware Train-Test Split\n",
    "We split the dataset into training and test sets using 'GroupShuffleSplit' to ensure no subject appears in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af38814",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['motor_UPDRS', 'total_UPDRS'])\n",
    "y = df[\"motor_UPDRS\"]  # or \"total_UPDRS\"\n",
    "groups = df[\"subject#\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a1812",
   "metadata": {},
   "source": [
    "## Exploratory data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c9b7d4",
   "metadata": {},
   "source": [
    "#### Plot distributions of numeric features\n",
    "\n",
    "Histograms are used to observe the distribution and spread of numerical features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Plotting feature distributions...\")\n",
    "X_train.drop(columns=['subject#']).hist(figsize=(15, 12), bins=30, color='skyblue', edgecolor='black')\n",
    "plt.suptitle(\"Feature Distributions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd977eda",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap \n",
    "\n",
    "Correlation heatmap is used to identify relationships between features and detect potential multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c56ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features and target for correlation\n",
    "train_data = X_train.copy()\n",
    "train_data[\"motor_UPDRS\"] = df[\"motor_UPDRS\"]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_data.drop(columns=['subject#']).corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "ax = sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    square=True,\n",
    "    cbar_kws={\n",
    "        'label': 'Correlation Strength',\n",
    "        'shrink': 0.9,\n",
    "        'ticks': [-1, -0.5, 0, 0.5, 1]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Label colorbar meaning\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.ax.set_yticklabels([\n",
    "    '-1: Strong Negative',\n",
    "    '-0.5: Moderate Negative',\n",
    "    '0: No Correlation',\n",
    "    '+0.5: Moderate Positive',\n",
    "    '+1: Strong Positive'\n",
    "])\n",
    "\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc6818",
   "metadata": {},
   "source": [
    "#### Violin plot\n",
    "Violin plot is used to visualize the distribution and density of a feature across severity classes.  \n",
    "It helps reveal differences in value spread, central tendency, and potential class-separating patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age Groups\n",
    "train_data[\"age_group\"] = pd.cut(train_data[\"age\"], bins=[0, 50, 60, 70, 80, 100], labels=[\"<50\", \"50-60\", \"60-70\", \"70-80\", \"80+\"])\n",
    "\n",
    "# Motor UPDRS by Sex\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(data=train_data, x=\"sex\", y=\"motor_UPDRS\")\n",
    "plt.title(\"Motor UPDRS by Sex\")\n",
    "plt.xticks([0, 1], ['Male', 'Female'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Motor UPDRS by Age Group\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.violinplot(data=train_data, x=\"age_group\", y=\"motor_UPDRS\", palette=\"Set2\")\n",
    "plt.title(\"Motor UPDRS by Age Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51132a",
   "metadata": {},
   "source": [
    "#### 4. Trend Heatmap \n",
    "Avg UPDRS over Time per Age Group (with labeled colorbar)\n",
    "\n",
    "While overall distributions give a general view, class-wise plots (e.g., violin/histogram by severity) help identify features that distinguish between classes for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin test_time into 10 equal-width bins\n",
    "train_data[\"time_bin\"] = pd.cut(train_data[\"test_time\"], bins=10)\n",
    "\n",
    "# Group and compute mean motor_UPDRS\n",
    "heatmap_data = train_data.groupby([\"age_group\", \"time_bin\"])[\"motor_UPDRS\"].mean().reset_index()\n",
    "\n",
    "# Pivot for heatmap\n",
    "heatmap_pivot = heatmap_data.pivot(index=\"age_group\", columns=\"time_bin\", values=\"motor_UPDRS\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.heatmap(heatmap_pivot, cmap=\"YlOrRd\", annot=True, fmt=\".1f\", cbar_kws={\n",
    "    'label': 'Average Motor UPDRS',\n",
    "    'shrink': 0.9\n",
    "})\n",
    "\n",
    "# Customize colorbar tick labels\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.ax.set_yticklabels([f\"{v:.0f}\" for v in colorbar.get_ticks()])  # Rounded values\n",
    "\n",
    "plt.title(\"Mean Motor UPDRS Over Time by Age Group \")\n",
    "plt.xlabel(\"Test Time Bin\")\n",
    "plt.ylabel(\"Age Group\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761ed9d",
   "metadata": {},
   "source": [
    "### Rapid Progression Analysis â€“ Overview\n",
    "\n",
    "- This section identifies patients who show a steep increase in motor symptom severity ('motor_UPDRS') over time.  Detecting rapid progression helps profile high-risk individuals, and could improve early intervention or personalized treatment models.  \n",
    "- We track how 'motor_UPDRS' changes per subject and flag cases with steep trends.\n",
    "- We first sort the dataset by 'subject#' and 'test_time' to ensure the progression is tracked chronologically for each patient. This is essential before applying rolling or difference-based analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238903e",
   "metadata": {},
   "source": [
    "#### Step 1: Filter out recently improving subjects\n",
    "- We define a 30-day progression threshold based on a yearly rate of 4.6 motor_UPDRS points.  \n",
    "- Subjects who have recently improved (i.e., decreasing trend in the last 30 days) are filtered out using is_recently_decreasing().  \n",
    "- Only those still progressing are retained for further rapid progression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RAPID PROGRESSION DETECTION â€“ 30-days WINDOW-BASED\n",
    "\n",
    "train_subjects = train_data[\"subject#\"].unique()\n",
    "\n",
    "# Define progression threshold\n",
    "fast_motor_progression_per_year = 4.6 \n",
    "window_days = 30\n",
    "motor_threshold = fast_motor_progression_per_year / 365 * window_days\n",
    "top_n = 10\n",
    "\n",
    "def is_recently_decreasing(group, col, tail_days=30):\n",
    "    recent = group[group[\"test_time\"] >= (group[\"test_time\"].max() - tail_days)]\n",
    "    return recent[col].iloc[-1] < recent[col].iloc[0]\n",
    "\n",
    "filtered_subjects = []\n",
    "for subject in train_subjects:\n",
    "    group = train_data[train_data[\"subject#\"] == subject].sort_values(\"test_time\")\n",
    "    if not is_recently_decreasing(group, \"motor_UPDRS\"):\n",
    "        filtered_subjects.append(subject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826da3",
   "metadata": {},
   "source": [
    "#### Step 2: Detect 30-day rapid progression spikes (motor only)\n",
    "- We define has_rapid_progression to check if a subject shows a sharp increase in motor_UPDRS within any 30-day window (Â±2 days).  \n",
    "- For each subject, we compare all test point pairs and flag those with a change exceeding the threshold.  \n",
    "- Subjects meeting this condition are added to rapid_motor_progressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_rapid_progression(group, col, threshold):\n",
    "    times = group[\"test_time\"].values\n",
    "    values = group[col].values\n",
    "    for i in range(len(times)):\n",
    "        for j in range(i + 1, len(times)):\n",
    "            delta_days = times[j] - times[i]\n",
    "            if window_days - 2 < delta_days <= window_days + 2:\n",
    "                delta_val = values[j] - values[i]\n",
    "                if delta_val > threshold:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "rapid_motor_progressors = []\n",
    "for subject in filtered_subjects:\n",
    "    group = train_data[train_data[\"subject#\"] == subject].sort_values(\"test_time\")\n",
    "    if has_rapid_progression(group, \"motor_UPDRS\", motor_threshold):\n",
    "        rapid_motor_progressors.append(subject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4e14a",
   "metadata": {},
   "source": [
    "#### Step 3: Compute Slopes and Rank Top Subjects\n",
    "\n",
    "- We define compute_slope using linear regression to calculate the rate of change in motor_UPDRS over time.  \n",
    "- We apply it to each rapid progressor to get their progression slope and rank them.  \n",
    "- The top N subjects with the steepest slopes are selected as the fastest progressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d36925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_slope(x, y):\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "    y = np.array(y)\n",
    "    if len(x) < 2:\n",
    "        return 0\n",
    "    return LinearRegression().fit(x, y).coef_[0]\n",
    "\n",
    "motor_slopes = {\n",
    "    subject: compute_slope(train_data[train_data[\"subject#\"] == subject][\"test_time\"],\n",
    "                           train_data[train_data[\"subject#\"] == subject][\"motor_UPDRS\"])\n",
    "    for subject in rapid_motor_progressors\n",
    "}\n",
    "\n",
    "top_motor_subjects = sorted(motor_slopes, key=motor_slopes.get, reverse=True)[:top_n]\n",
    "\n",
    "time_range = train_data.groupby(\"subject#\")[\"test_time\"].apply(lambda x: x.max() - x.min())\n",
    "print(f\"\\nTime range (minâ€“max): {time_range.min():.2f} â€“ {time_range.max():.2f} days\")\n",
    "print(f\"30-day Motor UPDRS Threshold: {motor_threshold:.2f}\")\n",
    "print(f\"Top {top_n} Rapid Motor Progressors: {top_motor_subjects}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cdff0c",
   "metadata": {},
   "source": [
    "#### Step 4: Plot the Progression\n",
    "\n",
    "We use Plotly to visualize the `motor_UPDRS` progression over time for the top N fastest progressors.  \n",
    "Each line represents a subject, showing how quickly their scores are rising across test days.  \n",
    "This interactive plot helps validate the rapid progression detection and provides clear insight into patient deterioration trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for subject in top_motor_subjects:\n",
    "    patient_data = train_data[train_data[\"subject#\"] == subject].sort_values(\"test_time\")\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=patient_data[\"test_time\"],\n",
    "        y=patient_data[\"motor_UPDRS\"],\n",
    "        mode='lines+markers',\n",
    "        name=f\"Subject {subject}\",\n",
    "        hoverinfo=\"text\",\n",
    "        text=[f\"Subject: {subject}<br>Time: {t:.1f}<br>Motor UPDRS: {u:.2f}\" for t, u in zip(patient_data[\"test_time\"], patient_data[\"motor_UPDRS\"])]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Top {top_n} Motor UPDRS Progressors (30-Day Spike, Training Set)\",\n",
    "    xaxis_title=\"Test Time (Days)\",\n",
    "    yaxis_title=\"Motor UPDRS\",\n",
    "    hovermode=\"closest\",\n",
    "    template=\"plotly_white\",\n",
    "    width=1000, height=600\n",
    ")\n",
    "fig.show()\n",
    "# Save interactive plot as HTML file (fully self-contained)\n",
    "#pyo.plot(fig, filename='top_10_updrs_progressors.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6146cf57",
   "metadata": {},
   "source": [
    "### Rolling average per subject\n",
    "\n",
    "- Calculateed a 3-point rolling average of motor_UPDRS for each subject using rolling(window=3).  \n",
    "- This smooths out short-term fluctuations in symptom scores and helps highlight the underlying progression trend more clearly.\n",
    "\n",
    "- Randomly selected 5 subjects from the training set and plot their rolling average motor_UPDRS over time.  \n",
    "- This provides a visual understanding of how symptoms evolve and how rolling averages reduce noise in progression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute rolling average per subject \n",
    "rolling_df = []\n",
    "\n",
    "for subject, group in train_data.groupby(\"subject#\"):\n",
    "    group_sorted = group.sort_values(\"test_time\").copy()\n",
    "    group_sorted[\"motor_UPDRS_rollmean\"] = group_sorted[\"motor_UPDRS\"].rolling(window=3, min_periods=1).mean()\n",
    "    rolling_df.append(group_sorted)\n",
    "\n",
    "df_rolling = pd.concat(rolling_df)\n",
    "\n",
    "# Step 2: Plot rolling average for 5 random subjects in training set\n",
    "sample_subjects = train_data[\"subject#\"].drop_duplicates().sample(5, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for subj in sample_subjects:\n",
    "    temp = df_rolling[df_rolling[\"subject#\"] == subj]\n",
    "    plt.plot(temp[\"test_time\"], temp[\"motor_UPDRS_rollmean\"], label=f\"Subject {subj}\")\n",
    "\n",
    "plt.title(\"Rolling Average of Motor UPDRS (Window=3)\")\n",
    "plt.xlabel(\"Test Time (Days)\")\n",
    "plt.ylabel(\"Motor UPDRS (Rolling Mean)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ec254",
   "metadata": {},
   "source": [
    "### 30-Day Delta Changes in Motor UPDRS\n",
    "- Here iterated through each subject and compute the change in motor_UPDRS across all available 30-day windows (Â±2 days).  \n",
    "- Only the first matching pair for each starting point is used to avoid multiple overlapping windows.  \n",
    "- This quantifies how much each patientâ€™s symptoms changed in typical 1-month periods.\n",
    "- plot a histogram of the 30-day delta values to observe the distribution of symptom changes.  \n",
    "- This shows how frequently patients improve, worsen, or remain stable over 1-month intervals, giving insights into short-term progression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fca095",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_motor = []\n",
    "\n",
    "# Loop through each subject in training data\n",
    "for subject, group in train_data.groupby(\"subject#\"):\n",
    "    group = group.sort_values(\"test_time\").reset_index(drop=True)\n",
    "    for i in range(len(group)):\n",
    "        for j in range(i + 1, len(group)):\n",
    "            delta_t = group.loc[j, \"test_time\"] - group.loc[i, \"test_time\"]\n",
    "            if 28 <= delta_t <= 32:\n",
    "                delta_motor.append(group.loc[j, \"motor_UPDRS\"] - group.loc[i, \"motor_UPDRS\"])\n",
    "                break  # Only take the first 30-day window delta for each i\n",
    "\n",
    "# Plot histogram of delta changes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(delta_motor, bins=30, color='teal', edgecolor='black')\n",
    "plt.title(\"Î” Motor UPDRS Over 30-Day Windows (Training Set)\")\n",
    "plt.xlabel(\"Change in Motor UPDRS\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8d7b2",
   "metadata": {},
   "source": [
    "### Severity Distribution Analysis\n",
    "- Clinical severity levels were defined based on motor_UPDRS score ranges: 0â€“20 for Mild and 21â€“40 for Moderate+.\n",
    "- Subjects were filtered into severity categories, and counts were computed using conditional indexing within a loop.\n",
    "\n",
    "\n",
    "##### Severity Label Encoding for Classification and distribution\n",
    "The continuous motor_UPDRS scores were converted into binary severity labels:\n",
    "- \"Mild\" for scores â‰¤ 20\n",
    "- \"Moderate+\" for scores > 20\n",
    "\n",
    "These labels were then encoded into integers using LabelEncoder to prepare for classification modeling.\n",
    "Class distribution in y_train_encoded was checked to verify label balance after encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop duplicates to count per subject\n",
    "train_unique = df.iloc[train_idx].drop_duplicates(\"subject#\")\n",
    "\n",
    "# 2. Define severity bins and count per group\n",
    "severity_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Severity Level\": label,\n",
    "        \"Motor UPDRS Count\": train_unique.query(f\"motor_UPDRS >= {low} and motor_UPDRS <= {high}\").shape[0]\n",
    "    }\n",
    "    for label, (low, high) in {\"Mild\": (0, 20), \"Moderate+\": (21, 40)}.items()\n",
    "])\n",
    "\n",
    "# 3. Plot bar chart\n",
    "plt.figure(figsize=(4, 5))\n",
    "ax = sns.barplot(data=severity_df, x=\"Severity Level\", y=\"Motor UPDRS Count\", palette=\"pastel\")\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2, p.get_height(), f\"{int(p.get_height())}\", \n",
    "            ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "plt.title(\"Motor UPDRS Severity Level Distribution (Train Set)\")\n",
    "plt.ylabel(\"Number of Subjects\")\n",
    "plt.xlabel(\"Severity Level\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Convert to binary labels + encode\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train.apply(lambda x: \"Mild\" if x <= 20 else \"Moderate+\"))\n",
    "y_test_encoded = le.transform(y_test.apply(lambda x: \"Mild\" if x <= 20 else \"Moderate+\"))\n",
    "\n",
    "# 5. Show class counts\n",
    "print(\"Class Distribution in y_train (Binary):\")\n",
    "print(pd.Series(y_train_encoded).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01cc207",
   "metadata": {},
   "source": [
    "## Feature Engineering and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9725c",
   "metadata": {},
   "source": [
    "### Domain based feature engineering and Scaling\n",
    "- Applies domain-driven transformations to input data.\n",
    "- log, sqrt, interaction, and shimmer slope features used in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db538e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [\n",
    "    \"age\", \"sex\", \"Jitter(%)\", \"Jitter(Abs)\", \"Jitter:RAP\", \"Jitter:PPQ5\", \"Jitter:DDP\",\n",
    "    \"Shimmer\", \"Shimmer(dB)\", \"Shimmer:APQ3\", \"Shimmer:APQ5\", \"Shimmer:APQ11\", \"Shimmer:DDA\",\n",
    "    \"NHR\", \"HNR\", \"RPDE\", \"DFA\", \"PPE\"]\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Derived features\n",
    "    df[\"log_PPE\"] = np.log1p(df[\"PPE\"])\n",
    "    df[\"Inv_JitterAbs\"] = 1 / (df[\"Jitter(Abs)\"] + 1e-6)\n",
    "    df[\"Shimmer_Slope\"] = df[\"Shimmer:APQ11\"] - (df[\"Shimmer:APQ3\"] + 1e-6)\n",
    "    df[\"Age_Sex_Interaction\"] = df[\"age\"] * df[\"sex\"]\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer to use in pipeline\n",
    "engineer_transformer = FunctionTransformer(engineer_features, validate=False)\n",
    "\n",
    "# Based on correlation and distribution EDA and Avoiding redundancy \n",
    "final_features = [\n",
    "    \"age\", \"sex\", \"Shimmer_Slope\", \"Inv_JitterAbs\",\n",
    "    \"log_PPE\", \"RPDE\", \"DFA\", \"Age_Sex_Interaction\"\n",
    "]\n",
    "\n",
    "select_features_transformer = FunctionTransformer(lambda df: df[final_features], validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac1ec0",
   "metadata": {},
   "source": [
    "#### Selected Feature Justification\n",
    "The following features were selected based on clinical relevance, exploratory analysis, and engineered signal strength:\n",
    "- **age**: A key demographic factor often correlated with progression severity.\n",
    "- **sex**: Included to account for differences in vocal traits across genders.\n",
    "- **Shimmer_Slope**: Reflects difference between short- and long-term shimmer measures (APQ11 - APQ3), indicating vocal instability.\n",
    "- **Inv_JitterAbs**: The inverse of jitter (1 / Jitter:Abs) emphasizes vocal stability, with higher values indicating more regular vocal fold vibrations, thus serving as a clinically interpretable marker of healthier voice function.\n",
    "- **RPDE**: Lower RPDE values indicate less periodicity and higher noise â€” a hallmark of dysphonia in Parkinsonâ€™s speech, especially as severity increases.\n",
    "- **log_PPE**: Log-transformed Pitch Period Entropy; stabilizes skew and highlights pitch irregularity.\n",
    "- **DFA**: High DFA values reflect more stable and self-similar patterns in vocal signals â€” typically preserved in early stages but lost as motor control worsens.\n",
    "\n",
    "These features balance interpretability and predictive value, making them suitable for modeling motor symptom severity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edecc12e",
   "metadata": {},
   "source": [
    "### Feature Scaling \n",
    "- We apply RobustScaler to voice features to normalize them without being skewed by extreme outliers.\n",
    "- Unlike standard scaling, it uses the median and interquartile range (IQR), making it ideal for jitter, shimmer, and PPE which can have sharp spikes in Parkinsonâ€™s speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler():\n",
    "    return RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf6823",
   "metadata": {},
   "source": [
    "## Modelling - Severity Level Prediction (Mild and Moderate+)\n",
    "### Cross-Validation Evaluation of Three Classifiers Using Pipelines\n",
    "\n",
    "- Three classification models (Logistic Regression, Random Forest, and XGBoost) were wrapped in Pipeline objects, each with 'RobustScaler' applied in pipeline step.  \n",
    "This ensures consistent feature scaling before model fitting.\n",
    "\n",
    "- 'StratifiedGroupKFold' was used to perform 5-fold cross-validation, preserving both class balance and group independence across folds.  \n",
    "For each model, predictions were collected across all validation folds and combined to evaluate performance.\n",
    "\n",
    "- The final classification report for each model summarizes overall precision, recall, and F1-score across all folds using the encoded severity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define group labels for cross-validation\n",
    "groups_train = df.iloc[train_idx][\"subject#\"]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define model pipelines with PCA step added\n",
    "clf_pipelines = {\n",
    "    \"Logistic\": Pipeline([\n",
    "        (\"engineer\", engineer_transformer),\n",
    "        (\"select\", select_features_transformer),\n",
    "        (\"scale\", get_scaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        (\"engineer\", engineer_transformer),\n",
    "        (\"select\", select_features_transformer),\n",
    "        (\"scale\", get_scaler()),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42))\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"engineer\", engineer_transformer),\n",
    "        (\"select\", select_features_transformer),\n",
    "        (\"scale\", get_scaler()),\n",
    "        (\"clf\", XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# Stratified group-aware cross-validation\n",
    "cv = StratifiedGroupKFold(n_splits=5)\n",
    "cv_clf_results_all = {}\n",
    "\n",
    "# Evaluate each classifier\n",
    "for model_name, pipeline in clf_pipelines.items():\n",
    "    clf_preds = []\n",
    "\n",
    "    for tr_idx, val_idx in cv.split(X_train, y_train_encoded, groups=groups_train):\n",
    "        X_tr, X_val = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train_encoded[tr_idx], y_train_encoded[val_idx]\n",
    "\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        fold_df = pd.DataFrame({\n",
    "            \"Actual_Severity\": y_val,\n",
    "            \"Predicted_Severity\": y_pred\n",
    "        }, index=X_val.index)\n",
    "\n",
    "        clf_preds.append(fold_df)\n",
    "\n",
    "    # Store aggregated results for this model\n",
    "    cv_clf_results_all[model_name] = pd.concat(clf_preds, ignore_index=True) \n",
    "    \n",
    "# Print out performance metrics\n",
    "for name, result_df in cv_clf_results_all.items():\n",
    "    print(f\"\\n{name} Classification Performance:\")\n",
    "    print(classification_report(result_df[\"Actual_Severity\"], result_df[\"Predicted_Severity\"], target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad3836d",
   "metadata": {},
   "source": [
    "### Model Comparison Summary (Cross-Validation)\n",
    "\n",
    "- Macro-averaged accuracy, precision, recall, Sensitivity(TPR), Specificity(TNR) and F1-scores were computed for each model using cross-validation results.  \n",
    "- This summary table supports model selection based on consistent performance across folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "summary_metrics = []\n",
    "\n",
    "# Loop over the collected CV results\n",
    "for model_name, result_df in cv_clf_results_all.items():\n",
    "    y_true = result_df[\"Actual_Severity\"]\n",
    "    y_pred = result_df[\"Predicted_Severity\"]\n",
    "\n",
    "    # Confusion matrix components\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    summary_metrics.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": round(accuracy_score(y_true, y_pred), 3),\n",
    "        \"Precision (Macro)\": round(precision_score(y_true, y_pred, average=\"macro\"), 3),\n",
    "        \"Recall (Macro)\": round(recall_score(y_true, y_pred, average=\"macro\"), 3),\n",
    "        \"F1-Score (Macro)\": round(f1_score(y_true, y_pred, average=\"macro\"), 3),\n",
    "        \"Sensitivity (TPR)\": round(tp / (tp + fn), 3),\n",
    "        \"Specificity (TNR)\": round(tn / (tn + fp), 3)\n",
    "    })\n",
    "\n",
    "# Create and display summary DataFrame\n",
    "comparison_df = pd.DataFrame(summary_metrics).sort_values(by=\"F1-Score (Macro)\", ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison Summary (Cross-Validation):\")\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe80b1",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Using GridSearchCV with Pipelines\n",
    "\n",
    "To optimize model performance, hyperparameter tuning was performed for Logistic Regression, Random Forest, and XGBoost using 'GridSearchCV'.  \n",
    "Each model was wrapped in a 'Pipeline' with a 'RobustScaler' to ensure consistent preprocessing.\n",
    "\n",
    "The 'tune_model' function takes a model pipeline, parameter grid, training data, and group-aware cross-validation strategy ('StratifiedGroupKFold').  \n",
    "It evaluates each parameter combination using 5-fold cross-validation, scoring by macro-averaged F1-score to handle class imbalance.\n",
    "\n",
    "Custom parameter grids were defined for each model, and the best estimator (i.e., best set of parameters) was selected and stored for future evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33569231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Function to run GridSearchCV on a pipeline ===\n",
    "def tune_model(pipeline, param_grid, X, y, groups, cv):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y, groups=groups)\n",
    "    return grid_search\n",
    "\n",
    "# === 2. Define parameter grids for each model ===\n",
    "param_grids = {\n",
    "    \"Logistic\": {\n",
    "        \"clf__C\": [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"clf__n_estimators\": [100, 200],\n",
    "        \"clf__max_depth\": [None, 5, 10],\n",
    "        \"clf__min_samples_split\": [2, 5]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"clf__n_estimators\": [100, 200],\n",
    "        \"clf__max_depth\": [3, 5],\n",
    "        \"clf__learning_rate\": [0.01, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === 3. Perform tuning for each model ===\n",
    "best_estimators = {}\n",
    "cv_scores = {}\n",
    "\n",
    "for model_name, pipeline in clf_pipelines.items():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    grid = tune_model(pipeline, param_grids[model_name], X_train, y_train_encoded, groups_train, cv)\n",
    "    best_estimators[model_name] = grid.best_estimator_\n",
    "    cv_scores[model_name] = round(grid.best_score_, 4)\n",
    "    print(f\"Best Params for {model_name}: {grid.best_params_}\")\n",
    "    print(f\"Best F1 Score: {cv_scores[model_name]}\")\n",
    "\n",
    "# === 4. Create combined model dictionary for training evaluation ===\n",
    "tuned_models = {\n",
    "    \"Logistic Regression\": (best_estimators[\"Logistic\"], cv_scores[\"Logistic\"]),\n",
    "    \"Random Forest\": (best_estimators[\"RandomForest\"], cv_scores[\"RandomForest\"]),\n",
    "    \"XGBoost\": (best_estimators[\"XGBoost\"], cv_scores[\"XGBoost\"])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ec828",
   "metadata": {},
   "source": [
    "### Training Set Evaluation of Tuned Models\n",
    "\n",
    "Each best estimator (from hyperparameter tuning) was retrained on the full training set and evaluated to assess model fit.\n",
    "\n",
    "The evaluate_on_train function computes:\n",
    "- **Classification report** including precision, recall, and F1-scores\n",
    "- **Macro F1-score and accuracy** on the training set\n",
    "- Comparison with the **cross-validation (CV) F1-score** to assess overfitting risk\n",
    "- **Confusion matrix** to visualize prediction distribution\n",
    "\n",
    "A small gap between training and CV performance indicates good generalization, while a large gap may suggest overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67337f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_train(model_name, model, X_train, y_train, label_encoder, cv_f1_score=None):\n",
    "    print(f\"\\n\\n=== {model_name} Performance on Training Set ===\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_train)\n",
    "    \n",
    "    print(classification_report(y_train, y_pred, target_names=label_encoder.classes_))\n",
    "    print(\"Accuracy:\", round(accuracy_score(y_train, y_pred), 3))\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_train, y_pred))\n",
    "    \n",
    "    if cv_f1_score:\n",
    "        print(\"CV F1-Score (Macro):\", round(cv_f1_score, 3))\n",
    "\n",
    "# Loop through models with F1-score\n",
    "for model_name, (model, cv_f1) in tuned_models.items():\n",
    "    evaluate_on_train(model_name, model, X_train, y_train_encoded, le, cv_f1_score=cv_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef6437",
   "metadata": {},
   "source": [
    "#### Tuned Model Performance Summary\n",
    "\n",
    "This table summarizes the performance of all three tuned models on the training set using key classification metrics.  \n",
    "It includes macro-averaged scores, sensitivity (recall for Moderate+), specificity (recall for Mild), and cross-validated F1-scores for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8268dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_metrics = []\n",
    "\n",
    "for model_name, (model, cv_f1_score_val) in tuned_models.items():\n",
    "    model.fit(X_train, y_train_encoded)\n",
    "    y_pred = model.predict(X_train)\n",
    "\n",
    "    acc = accuracy_score(y_train_encoded, y_pred)\n",
    "    precision = precision_score(y_train_encoded, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_train_encoded, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_train_encoded, y_pred, average=\"macro\")\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_train_encoded, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    train_summary_metrics.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": round(acc, 3),\n",
    "        \"Precision (Macro)\": round(precision, 3),\n",
    "        \"Recall (Macro)\": round(recall, 3),\n",
    "        \"F1-Score (Macro)\": round(f1, 3),\n",
    "        \"Sensitivity (TPR)\": round(sensitivity, 3),\n",
    "        \"Specificity (TNR)\": round(specificity, 3),\n",
    "        \"CV F1-Score\": round(cv_f1_score_val, 3)\n",
    "    })\n",
    "\n",
    "# === 3. Display comparison table ===\n",
    "comparison_df = pd.DataFrame(train_summary_metrics).sort_values(by=\"F1-Score (Macro)\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Tuned Models: Training Performance Summary ===\")\n",
    "display(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb64b3b",
   "metadata": {},
   "source": [
    "### Final Evaluation on Held-Out Test Set (Logistic Regression)\n",
    "\n",
    "The best-performing model from tuning (Logistic Regression) was used to make predictions on the held-out test set.\n",
    "\n",
    "The following metrics were computed:\n",
    "- **Classification Report**: Shows precision, recall, and F1-score per class\n",
    "- **Accuracy**: Overall classification accuracy\n",
    "- **Confusion Matrix**: Displays the distribution of predicted vs actual labels\n",
    "\n",
    "This evaluation provides an unbiased estimate of real-world model performance, as the test set was never used during training or tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ff623a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, X_test, y_test, label_encoder, model_name=\"Model\"):\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    # Handle confusion matrix safely\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.shape == (2, 2) else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    # Display detailed performance report\n",
    "    print(f\"=== {model_name} â€“ Held-Out Test Set Performance ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    print(f\"Accuracy: {round(acc, 3)}\")\n",
    "    print(f\"Precision (Macro): {round(precision, 3)}\")\n",
    "    print(f\"Recall (Macro): {round(recall, 3)}\")\n",
    "    print(f\"F1-Score (Macro): {round(f1, 3)}\")\n",
    "    print(f\"Sensitivity (TPR): {round(sensitivity, 3)}\")\n",
    "    print(f\"Specificity (TNR): {round(specificity, 3)}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Evaluating on the Logistic Regression model on the test set\n",
    "evaluate_model_on_test(\n",
    "    model=best_estimators[\"Logistic\"],\n",
    "    X_test=X_test,  # raw test set (will go through pipeline inside)\n",
    "    y_test=y_test_encoded,\n",
    "    label_encoder=le,\n",
    "    model_name=\"Logistic Regression\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59315a59",
   "metadata": {},
   "source": [
    "#### Interpretation \n",
    "\n",
    "- The model achieved an accuracy of 60% and a macro F1-score of 0.59 on the held-out test set.  \n",
    "- This is close to the CV F1-score (0.56), which is a good sign â€” it suggests the model generalized well and is not overfitting.  - Performance is balanced, though slightly better on the 'Mild' class, indicating room for improvement in distinguishing 'Moderate+' cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0fc98",
   "metadata": {},
   "source": [
    "## Logistic Regression Feature Importance\n",
    "\n",
    "To interpret how each feature contributes to the prediction, the model coefficients from the best Logistic Regression estimator were extracted.  \n",
    "These coefficients indicate the direction and strength of influence each feature has on the predicted severity class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e4e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Access logistic model from pipeline ===\n",
    "# Ensure you're using the correct best estimator (not redefined)\n",
    "logistic_model = best_estimators[\"Logistic\"].named_steps[\"clf\"]\n",
    "\n",
    "# === Extract coefficients for each feature ===\n",
    "logit_coefs = logistic_model.coef_[0]\n",
    "\n",
    "# === Plotting feature importance based on coefficients ===\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.barh(final_features, logit_coefs)\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Logistic Regression Feature Importance (Raw Coefficients)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cdf68",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Positive coefficients indicate features associated with higher severity (Moderate+), while negative values are linked to the Mild class.  \n",
    "This helps identify which vocal and clinical factors are most influential in the model's decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbfece",
   "metadata": {},
   "source": [
    "###  X_train_final and X_test_final from pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1952e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessing pipeline from best estimator\n",
    "preprocessing = best_estimators[\"Logistic\"].named_steps\n",
    "\n",
    "# Transform training and test data manually \n",
    "X_train_final = preprocessing[\"scale\"].transform(\n",
    "    preprocessing[\"select\"].transform(\n",
    "        preprocessing[\"engineer\"].transform(X_train.copy())\n",
    "    )\n",
    ")\n",
    "\n",
    "X_test_final = preprocessing[\"scale\"].transform(\n",
    "    preprocessing[\"select\"].transform(\n",
    "        preprocessing[\"engineer\"].transform(X_test.copy())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d64158",
   "metadata": {},
   "source": [
    "### SHAP Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df305105",
   "metadata": {},
   "source": [
    "####  SHAP Analysis: Interpreting Feature Influence on Parkinson's Severity Prediction\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is used to interpret how each feature contributes to the model's prediction. This method is based on cooperative game theory and provides both **global** and **individual-level** explanations.\n",
    "\n",
    "In this section, we:\n",
    "1. Initialize a SHAP explainer for the trained logistic regression model.\n",
    "2. Compute SHAP values for the test set to understand feature contributions.\n",
    "3. Generate visualizations:\n",
    "   - **Bar Plot**: Global feature importance (mean absolute SHAP values)\n",
    "   - **Beeswarm Plot**: Distribution of individual SHAP values per feature across all test samples\n",
    "\n",
    "These plots help identify which features consistently drive predictions toward the \"Mild\" or \"Moderate+\" class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create SHAP explainer for linear model\n",
    "explainer = shap.Explainer(logistic_model, X_train_final, feature_names=final_features)\n",
    "\n",
    "# 2. Compute SHAP values for the held-out test set\n",
    "shap_values = explainer(X_test_final)\n",
    "\n",
    "# 3. Global Feature Importance (Bar plot)\n",
    "shap.summary_plot(shap_values, X_test_final, plot_type=\"bar\", show=True)\n",
    "\n",
    "# 4. Individual-Level Impact (Beeswarm plot)\n",
    "shap.summary_plot(shap_values, X_test_final, show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ae4e5",
   "metadata": {},
   "source": [
    "#### SHAP Interpretation Summary\n",
    "- **Age**\n",
    "Highest global impact on severity classification.\n",
    "Older individuals more likely predicted as Moderate+.\n",
    "Consistent with disease progression trends.\n",
    "\n",
    "- **DFA (Detrended Fluctuation Analysis)**\n",
    "High values reduce severity prediction.\n",
    "Reflects more stable vocal dynamics, typical of milder symptoms.\n",
    "\n",
    "- **Shimmer_Slope**\n",
    "Strong indicator of amplitude instability in voice.\n",
    "High values push prediction toward Moderate+.\n",
    "\n",
    "- **Age_Sex_Interaction**\n",
    "Moderate influence.\n",
    "Suggests age and sex jointly affect vocal degradation patterns.\n",
    "\n",
    "- **log_PPE**\n",
    "Higher pitch entropy = more vocal irregularity.\n",
    "Indicates impaired neuromuscular control seen in PD.\n",
    "\n",
    "- **RPDE**\n",
    "Subtle but meaningful.\n",
    "Captures voice periodicity, helps differentiate between healthy and impaired speech.\n",
    "\n",
    "- **Sex**\n",
    "Plays a role, but secondary to core acoustic markers.\n",
    "\n",
    "- **Inv_JitterAbs**\n",
    "New inverse jitter-based feature.\n",
    "Lower jitter (higher Inv_JitterAbs) contributes to Mild prediction.\n",
    "Acts as a stability marker in speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297996ab",
   "metadata": {},
   "source": [
    "###  SHAP Summary â€“ Misclassified Test Samples\n",
    "\n",
    "To better understand the model's limitations, we analyzed SHAP values specifically for **misclassified test cases** (i.e., where the predicted severity class didn't match the actual one). This helps identify which features were most misleading or contributed to confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a013d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict on already-transformed X_test_final (no need to transform again)\n",
    "logistic_model = best_estimators[\"Logistic\"].named_steps[\"clf\"]\n",
    "y_test_pred = logistic_model.predict(X_test_final)\n",
    "\n",
    "# 2. Identify misclassified samples\n",
    "misclassified_mask = (y_test_pred != y_test_encoded)\n",
    "\n",
    "# 3. Create SHAP explainer on training set\n",
    "explainer = shap.Explainer(logistic_model, X_train_final, feature_names=final_features)\n",
    "\n",
    "# 4. Compute SHAP values on the test set\n",
    "shap_values = explainer(X_test_final)\n",
    "\n",
    "# 5. Filter SHAP values for misclassified samples\n",
    "shap_misclassified = shap.Explanation(\n",
    "    values=shap_values.values[misclassified_mask],\n",
    "    base_values=shap_values.base_values[misclassified_mask],\n",
    "    data=X_test_final[misclassified_mask],\n",
    "    feature_names=final_features\n",
    ")\n",
    "\n",
    "# 6. Plot beeswarm for misclassified\n",
    "shap.summary_plot(shap_misclassified, X_test_final[misclassified_mask], plot_type=\"dot\", show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092afb83",
   "metadata": {},
   "source": [
    "#### Observations from the Beeswarm Plot (Misclassified Samples)\n",
    "\n",
    "- **DFA**\n",
    "Still dominant among errors.\n",
    "High values sometimes lead to incorrect Mild classification.\n",
    "\n",
    "- **Shimmer_Slope**\n",
    "High variability persists.\n",
    "Contributes both positively and negatively â€” inconsistent across individuals.\n",
    "\n",
    "- **Age_Sex_Interaction**\n",
    "Moderate role.\n",
    "Interactions seem to add ambiguity in borderline predictions.\n",
    "\n",
    "- **log_PPE and RPDE**\n",
    "Overlapping contributions across classes.\n",
    "May lack discriminative power alone in difficult cases.\n",
    "\n",
    "- **Sex**\n",
    "Lower influence overall, but shows direction flips.\n",
    "Potentially interacts with acoustic features in subtle ways.\n",
    "\n",
    "- **Inv_JitterAbs**\n",
    "Low inverse jitter still drives some Moderate+ predictions.\n",
    "Impact less clear in misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504c2ca",
   "metadata": {},
   "source": [
    "### LIME Interpretability â€“ Logistic Regression\n",
    "\n",
    "To improve interpretability at the individual prediction level, LIME (Local Interpretable Model-Agnostic Explanations) was applied to the best Logistic Regression model.  \n",
    "LIME explains a single prediction by approximating the model locally with an interpretable surrogate (e.g., linear model) and identifies the most influential features for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize the explainer\n",
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train_final,                # NumPy array\n",
    "    feature_names=final_features,               # Feature names after transform\n",
    "    class_names=le.classes_,                    # Class labels (\"Mild\", \"Moderate+\")\n",
    "    mode='classification',\n",
    "    discretize_continuous=True\n",
    ")\n",
    "\n",
    "#  Pick a few indices to explain\n",
    "sample_indices = np.random.choice(X_test_final.shape[0], 3, replace=False)\n",
    "\n",
    "#  Explain and display each\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nLIME Explanation for Test Sample Index: {idx}\")\n",
    "    \n",
    "    exp = lime_explainer.explain_instance(\n",
    "        data_row=X_test_final[idx],\n",
    "        predict_fn=logistic_model.predict_proba,\n",
    "        num_features=8\n",
    "    )\n",
    "    \n",
    "    exp.show_in_notebook(show_table=True)  # Or save as HTML\n",
    "    # exp.save_to_file(f\"lime_explanation_{i+1}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f9a7e",
   "metadata": {},
   "source": [
    "#### LIME Interpretation (Logistic Model â€“ Sample Cases)\n",
    "**Sample 155**\n",
    "- Prediction: Moderate+ (0.56)\n",
    "- Moderate+ Influencers: Higher age (>0.54), low DFA, and low Inv_JitterAbs\n",
    "- Mild Influencers: Lower log_PPE, neutral Age_Sex_Interaction, and RPDE > 0.50\n",
    "- Interpretation: Slightly above-threshold age and lowered pitch dynamics (DFA) led the model toward Moderate+, despite signs of acoustic stability.\n",
    "\n",
    "**Sample 1227**\n",
    "- Prediction: Moderate+ (0.88)\n",
    "- Moderate+ Influencers: Very high age (1.54), low DFA, high Age_Sex_Interaction\n",
    "- Mild Influencers: Low log_PPE and mildly elevated Shimmer_Slope\n",
    "- Interpretation: Clear prediction for Moderate+ due to multiple strong vocal and demographic indicators outweighing mild acoustic compensation.\n",
    "\n",
    "**Sample 335**\n",
    "- Prediction: Mild (0.58)\n",
    "- Mild Influencers: Low age, very low Shimmer_Slope, and high Inv_JitterAbs\n",
    "- Moderate+ Factors: High RPDE, male sex, and elevated Age_Sex_Interaction\n",
    "- Interpretation: Despite structural voice issues, younger age and vocal regularity drove the model toward a Mild outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac1442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
